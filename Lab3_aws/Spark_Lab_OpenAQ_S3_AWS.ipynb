{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee2cb67d",
   "metadata": {},
   "source": [
    "# Spark Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070a29b5",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fb5d857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>0</td><td>application_1762986994807_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-66-56.ec2.internal:20888/proxy/application_1762986994807_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-65-169.ec2.internal:8042/node/containerlogs/container_1762986994807_0001_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql.functions import col, asc, desc, max, hour, avg, date_format, rank\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import TimestampType, DateType\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbdee7b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "565c2039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1</td><td>application_1762986994807_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-66-56.ec2.internal:20888/proxy/application_1762986994807_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-79-169.ec2.internal:8042/node/containerlogs/container_1762986994807_0002_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.pyspark.python': 'python3', 'spark.pyspark.virtualenv.enabled': 'true', 'spark.pyspark.virtualenv.type': 'native', 'spark.pyspark.virtualenv.bin.path': '/usr/bin/virtualenv'}, 'proxyUser': 'jovyan', 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1</td><td>application_1762986994807_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-66-56.ec2.internal:20888/proxy/application_1762986994807_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-79-169.ec2.internal:8042/node/containerlogs/container_1762986994807_0002_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.pyspark.python\": \"python3\",\n",
    "        \"spark.pyspark.virtualenv.enabled\": \"true\",\n",
    "        \"spark.pyspark.virtualenv.type\":\"native\",\n",
    "        \"spark.pyspark.virtualenv.bin.path\":\"/usr/bin/virtualenv\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a513fb84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib64/python3.9/site-packages (2.2.3)\n",
      "Collecting tzdata>=2022.7\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Collecting numpy>=1.22.4\n",
      "  Downloading numpy-2.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
      "Collecting python-dateutil>=2.8.2\n",
      "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.13.0)\n",
      "Installing collected packages: tzdata, python-dateutil, numpy\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.8.1\n",
      "    Not uninstalling python-dateutil at /usr/lib/python3.9/site-packages, outside environment /mnt1/yarn/usercache/livy/appcache/application_1762986994807_0002/container_1762986994807_0002_01_000001/tmp/spark-7191e70f-e084-4ca3-8fd8-d0b7d681aafc\n",
      "    Can't uninstall 'python-dateutil'. No files were found to uninstall.\n",
      "Successfully installed numpy-2.0.2 python-dateutil-2.9.0.post0 tzdata-2025.2\n",
      "\n",
      "Requirement already satisfied: numpy in ./tmp/spark-7191e70f-e084-4ca3-8fd8-d0b7d681aafc/lib64/python3.9/site-packages (2.0.2)\n",
      "\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib64/python3.9/site-packages (3.9.4)\n",
      "Collecting importlib-resources>=3.2.0\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.60.1-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (4.8 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3.9/site-packages (from matplotlib) (21.3)\n",
      "Collecting kiwisolver>=1.3.1\n",
      "  Downloading kiwisolver-1.4.7-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "Requirement already satisfied: numpy>=1.23 in ./tmp/spark-7191e70f-e084-4ca3-8fd8-d0b7d681aafc/lib64/python3.9/site-packages (from matplotlib) (2.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3.9/site-packages (from matplotlib) (2.4.7)\n",
      "Collecting pillow>=8\n",
      "  Downloading pillow-11.3.0-cp39-cp39-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./tmp/spark-7191e70f-e084-4ca3-8fd8-d0b7d681aafc/lib/python3.9/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (321 kB)\n",
      "Collecting zipp>=3.1.0\n",
      "  Downloading zipp-3.23.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.13.0)\n",
      "Installing collected packages: zipp, pillow, kiwisolver, importlib-resources, fonttools, cycler, contourpy\n",
      "Successfully installed contourpy-1.3.0 cycler-0.12.1 fonttools-4.60.1 importlib-resources-6.5.2 kiwisolver-1.4.7 pillow-11.3.0 zipp-3.23.0\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "matplotlib 3.9.4 requires contourpy>=1.0.1, which is not installed.\n",
      "matplotlib 3.9.4 requires cycler>=0.10, which is not installed.\n",
      "matplotlib 3.9.4 requires fonttools>=4.22.0, which is not installed.\n",
      "matplotlib 3.9.4 requires importlib-resources>=3.2.0; python_version < \"3.10\", which is not installed.\n",
      "matplotlib 3.9.4 requires kiwisolver>=1.3.1, which is not installed.\n",
      "matplotlib 3.9.4 requires pillow>=8, which is not installed.\n",
      "awscli 2.30.4 requires python-dateutil<=2.9.0,>=2.1, but you have python-dateutil 2.9.0.post0 which is incompatible.\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag."
     ]
    }
   ],
   "source": [
    "sc.install_pypi_package(\"pandas\")\n",
    "sc.install_pypi_package(\"numpy\")\n",
    "sc.install_pypi_package(\"matplotlib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d0c88f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471d172c",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb1db97",
   "metadata": {},
   "source": [
    "Reading OpenAQ data from AWS S3\n",
    "\n",
    "OpenAQ project: https://openaq.org/\n",
    "\n",
    "Registry of AWS Open Data: https://registry.opendata.aws/openaq/\n",
    "\n",
    "S3 bucket structure: https://docs.openaq.org/aws/about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c38140a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "smog_df = spark \\\n",
    "    .read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"compression\", \"gzip\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .load(\"s3a://openaq-data-archive/records/csv.gz/locationid=100*/year=2022/month=05/location-100*-2022050*.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2afb8f69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500"
     ]
    }
   ],
   "source": [
    "smog_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "250df724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+--------------------------+-------------------------+----------------+----------------+---------+-----+------+\n",
      "|location_id|sensors_id|location                  |datetime                 |lat             |lon             |parameter|units|value |\n",
      "+-----------+----------+--------------------------+-------------------------+----------------+----------------+---------+-----+------+\n",
      "|100        |162       |Badhoevedorp-Sloterweg-100|2022-05-07T12:00:00+02:00|52.334          |4.77401         |co       |µg/m³|-999.0|\n",
      "|1000       |1803      |Presque Isle Riversi-1000 |2022-05-07T13:00:00-04:00|46.682299       |-68.016195      |pm10     |µg/m³|9.0   |\n",
      "|1003       |1806      |Padonia-1003              |2022-05-03T01:00:00-04:00|39.462002       |-76.631599      |o3       |ppm  |0.026 |\n",
      "|1003       |1806      |Padonia-1003              |2022-05-03T13:00:00-04:00|39.462002       |-76.631599      |o3       |ppm  |0.04  |\n",
      "|1003       |1806      |Padonia-1003              |2022-05-03T21:00:00-04:00|39.462002       |-76.631599      |o3       |ppm  |0.046 |\n",
      "|1004       |4662      |Padre Las Casas II-1004   |2022-05-09T22:00:00-01:00|-38.764767451719|-72.598795682209|pm10     |µg/m³|40.0  |\n",
      "|1008       |1817      |Palatka-1008              |2022-05-03T05:00:00-04:00|29.686667       |-81.656389      |pm10     |µg/m³|26.0  |\n",
      "|1002       |1805      |Pacific-1002              |2022-05-07T21:00:00-05:00|38.4902         |-90.7052        |o3       |ppm  |0.03  |\n",
      "+-----------+----------+--------------------------+-------------------------+----------------+----------------+---------+-----+------+"
     ]
    }
   ],
   "source": [
    "smog_df.sample(fraction=0.01).limit(10).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43714b6a",
   "metadata": {},
   "source": [
    "### Using extended locations data\n",
    "\n",
    "Data in the S3 bucket contains only a few necessary fields -- parameter readings and a limited information about sensor locations.\n",
    "\n",
    "Additional information are available through OpenAQ API: https://api.openaq.org/\n",
    "\n",
    "File openaq_locations.json contains extended information about locations which was downloaded using this REST API.\n",
    "\n",
    "Data from this file joined with data in S3 can be used for more advanced queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89786a7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------------+-------------+------------+--------+---+--------------------+--------+---------+--------+--------+-------------------+--------------------+--------------------+--------------------+------------+\n",
      "|              bounds|         coordinates|         country|datetimeFirst|datetimeLast|distance| id|         instruments|isMobile|isMonitor|licenses|locality|               name|               owner|            provider|             sensors|    timezone|\n",
      "+--------------------+--------------------+----------------+-------------+------------+--------+---+--------------------+--------+---------+--------+--------+-------------------+--------------------+--------------------+--------------------+------------+\n",
      "|[-0.19968, 5.5838...| {5.58389, -0.19968}|{GH, 152, Ghana}|         NULL|        NULL|    NULL|  3|[{2, Government M...|   false|     true|    NULL|    NULL|         NMA - Nima|{4, Unknown Gover...|{209, Dr. Raphael...|[{6, pm10 µg/m³, ...|Africa/Accra|\n",
      "|[-0.19898, 5.5816...| {5.58165, -0.19898}|{GH, 152, Ghana}|         NULL|        NULL|    NULL|  4|[{2, Government M...|   false|     true|    NULL|    NULL|         NMT - Nima|{4, Unknown Gover...|{209, Dr. Raphael...|[{7, pm10 µg/m³, ...|Africa/Accra|\n",
      "|[-0.2103972, 5.54...|{5.5401139, -0.21...|{GH, 152, Ghana}|         NULL|        NULL|    NULL|  5|[{2, Government M...|   false|     true|    NULL|    NULL|    JTA - Jamestown|{4, Unknown Gover...|{209, Dr. Raphael...|[{10, pm10 µg/m³,...|Africa/Accra|\n",
      "|[-0.2120555, 5.57...|{5.570722, -0.212...|{GH, 152, Ghana}|         NULL|        NULL|    NULL|  6|[{2, Government M...|   false|     true|    NULL|    NULL|  ADT - Asylum Down|{4, Unknown Gover...|{209, Dr. Raphael...|[{11, pm10 µg/m³,...|Africa/Accra|\n",
      "|[-0.2040278, 5.56...|{5.567833, -0.204...|{GH, 152, Ghana}|         NULL|        NULL|    NULL|  7|[{2, Government M...|   false|     true|    NULL|    NULL|ADEPA - Asylum Down|{4, Unknown Gover...|{209, Dr. Raphael...|[{14, pm10 µg/m³,...|Africa/Accra|\n",
      "+--------------------+--------------------+----------------+-------------+------------+--------+---+--------------------+--------+---------+--------+--------+-------------------+--------------------+--------------------+--------------------+------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"multiLine\", \"true\").json(\"s3a://openaqlocationsadzd/openaq_locations.json\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f2b0deb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "locations_df = df.select(\n",
    "    \"id\",\n",
    "    \"name\",\n",
    "    \"timezone\",\n",
    "    df[\"country.code\"].alias(\"country_code\"),\n",
    "    df[\"country.name\"].alias(\"country_name\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45159fbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-------------------+------------+-------------+\n",
      "|id  |name                |timezone           |country_code|country_name |\n",
      "+----+--------------------+-------------------+------------+-------------+\n",
      "|23  |Amgalan             |Asia/Ulaanbaatar   |MN          |Mongolia     |\n",
      "|300 |Glen Burnie         |America/New_York   |US          |United States|\n",
      "|403 |Museo Ferroviario   |America/Santiago   |CL          |Chile        |\n",
      "|539 |Pinedale            |America/Denver     |US          |United States|\n",
      "|634 |White Plains        |America/New_York   |US          |United States|\n",
      "|682 |BDED                |America/New_York   |US          |United States|\n",
      "|728 |Ronan-MT            |America/Denver     |US          |United States|\n",
      "|843 |HU-Beltsville       |America/New_York   |US          |United States|\n",
      "|916 |Wyong               |Australia/Sydney   |AU          |Australia    |\n",
      "|926 |UAM Xochimilco      |America/Mexico_City|MX          |Mexico       |\n",
      "|1038|Pilot Point C1032   |America/Chicago    |US          |United States|\n",
      "|1065|Powell River Wildwoo|America/Vancouver  |CA          |Canada       |\n",
      "|1189|Keeler              |America/Los_Angeles|US          |United States|\n",
      "|1343|BASKETT             |America/Chicago    |US          |United States|\n",
      "|1459|WY/KC               |America/Chicago    |US          |United States|\n",
      "|1480|Bangor KPS          |America/New_York   |US          |United States|\n",
      "|1570|Brainerd            |America/Chicago    |US          |United States|\n",
      "|1600|Burnaby South       |America/Vancouver  |CA          |Canada       |\n",
      "|1966|Sault Ste Marie     |America/Toronto    |CA          |Canada       |\n",
      "|2134|Oakland             |America/Los_Angeles|US          |United States|\n",
      "+----+--------------------+-------------------+------------+-------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "locations_df.sample(fraction=0.01).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8a3353c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- timezone: string (nullable = true)\n",
      " |-- country_code: string (nullable = true)\n",
      " |-- country_name: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "locations_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e82ad6b",
   "metadata": {},
   "source": [
    "## Big data cluster computations\n",
    "\n",
    "1. Perform calculations for task 2: Create a query that calculates the average concentration of a chosen parameter for each hour of the day (0-1, 1-2, ..., 23-24) for every location, and finds the hour at which this average was highest.\n",
    "2. Calculate time of execution for 2, 3, 4, 5, 6, 7 worker instances\n",
    "3. Create execution time, speedup and efficiency plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbed7de7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_valid_s3_paths(location_ids, spark_session, target_count=30):\n",
    "    valid_paths = []\n",
    "    for loc_id in location_ids:\n",
    "        path = f\"s3a://openaq-data-archive/records/csv.gz/locationid={loc_id}/year=2022/month={{05,06}}/*.csv.gz\"\n",
    "        try:\n",
    "            spark_session.read.format(\"csv\").option(\"compression\", \"gzip\").option(\"header\", True).load(path)\n",
    "            valid_paths.append(path)\n",
    "            #print(f\"Found valid path for location ID: {loc_id}\")\n",
    "        except Exception as e:\n",
    "            if \"PATH_NOT_FOUND\" in str(e):\n",
    "                pass\n",
    "            else:\n",
    "                print(f\"Error loading data for location ID {loc_id}: {e}\")\n",
    "\n",
    "        if len(valid_paths) >= target_count:\n",
    "            break\n",
    "\n",
    "    return valid_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80082ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def get_df_from_paths(s3_paths, spark_session, locations_df):\n",
    "    if not s3_paths:\n",
    "        return None\n",
    "\n",
    "    correct_dfs = []\n",
    "    for path in s3_paths:\n",
    "      dframes = spark_session \\\n",
    "          .read \\\n",
    "          .format(\"csv\") \\\n",
    "          .option(\"compression\", \"gzip\") \\\n",
    "          .option(\"header\", True) \\\n",
    "          .load(path)\n",
    "      correct_dfs.append(dframes)\n",
    "\n",
    "    df_all = reduce(DataFrame.unionByName, correct_dfs)\n",
    "\n",
    "    df_typed = (\n",
    "        df_all\n",
    "        .withColumn(\"value\", col(\"value\").cast(\"double\"))\n",
    "        .withColumn(\"location_id\", col(\"location_id\").cast(\"long\"))\n",
    "        .withColumn(\"datetime\", col(\"datetime\").cast(\"timestamp\"))\n",
    "    )\n",
    "\n",
    "    df_final = df_typed.join(\n",
    "        locations_df.select(\"id\", \"name\"),\n",
    "        df_typed.location_id == locations_df.id,\n",
    "        how='left'\n",
    "    ).drop(locations_df.id)\n",
    "\n",
    "    return df_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002d3eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "pl_locations_df = locations_df.filter(locations_df.country_code == 'PL').select('name', 'id')\n",
    "pl_location_ids = pl_locations_df.select(\"id\").rdd.flatMap(lambda x: x).collect()\n",
    "pl_100 = get_valid_s3_paths(pl_location_ids, spark, target_count=100)\n",
    "df_final = get_df_from_paths(pl_100, spark, pl_locations_df)\n",
    "\n",
    "PARAMETER = \"co\"\n",
    "\n",
    "hour_avg_co = df_final \\\n",
    "  .filter(col(\"parameter\") == PARAMETER) \\\n",
    "  .groupBy(\n",
    "    col(\"name\"),\n",
    "    hour(col(\"datetime\")).alias(\"hour\")\n",
    "  ) \\\n",
    "  .agg(avg(col(\"value\")).alias(\"avg_value\"))\n",
    "\n",
    "w = Window.partitionBy('name')\n",
    "\n",
    "hour_avg_co_with_max = hour_avg_co.withColumn('max_avg_value', max('avg_value').over(w))\n",
    "\n",
    "highest_avg_hour_per_location = hour_avg_co_with_max.filter(col('avg_value') == col('max_avg_value')).drop('max_avg_value')\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3aa4848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# workers = np.array[1, 2, 3, 4, 5, 6, 7]\n",
    "# times = np.array([0, 0, 0, 0, 0, 0, 0])\n",
    "# efficiencies = speedups / workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8db3ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4fe2b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
